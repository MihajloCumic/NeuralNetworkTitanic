{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_8dnbWqc9VX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_clean_data(path):\n",
        "  df = pd.read_csv(path)\n",
        "  df.drop(['PassengerId','Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
        "  df = df.dropna()\n",
        "  df.Sex = df.Sex.replace({'female': 0, 'male': 1})\n",
        "  df.Embarked = df.Embarked.replace({'S': 0, 'C':1, 'Q':2})\n",
        "  return df"
      ],
      "metadata": {
        "id": "Gi2GJahJdWZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = '/content/drive/MyDrive/Colab Notebooks/Domaci2/titanic/train.csv'\n",
        "\n",
        "df_train = load_clean_data(train_data_path)\n",
        "\n",
        "y_train = df_train.Survived.to_numpy()\n",
        "df_train.drop(['Survived'], axis=1, inplace=True)\n",
        "x_train = df_train.to_numpy()\n",
        "\n",
        "nb_train = len(x_train)\n",
        "\n",
        "learning_rate = 0.001\n",
        "nb_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "nb_input = 7\n",
        "nb_hidden1 = 256\n",
        "nb_hidden2 = 256\n",
        "nb_classes = 2\n",
        "\n",
        "w = {\n",
        "    '1': tf.Variable(tf.random.normal([nb_input, nb_hidden1], dtype=tf.float64)),\n",
        "    '2': tf.Variable(tf.random.normal([nb_hidden1, nb_hidden2], dtype=tf.float64)),\n",
        "    'out':tf.Variable(tf.random.normal([nb_hidden2, nb_classes], dtype=tf.float64))\n",
        "}\n",
        "\n",
        "b = {\n",
        "    '1': tf.Variable(tf.random.normal([nb_hidden1], dtype=tf.float64)),\n",
        "    '2': tf.Variable(tf.random.normal([nb_hidden2], dtype=tf.float64)),\n",
        "    'out': tf.Variable(tf.random.normal([nb_classes], dtype=tf.float64))\n",
        "}\n",
        "\n",
        "f = {\n",
        "    '1': tf.nn.relu,\n",
        "    '2': tf.nn.relu,\n",
        "    'out': tf.nn.softmax\n",
        "}\n",
        "\n",
        "def runNN(x):\n",
        "  z1 = tf.add(tf.matmul(x, w['1']), b['1'])\n",
        "  a1 = f['1'](z1)\n",
        "  z2 = tf.add(tf.matmul(a1, w['2']), b['2'])\n",
        "  a2 = f['2'](z2)\n",
        "  z_out = tf.add(tf.matmul(a2, w['out']), b['out'])\n",
        "  out = f['out'](z_out)\n",
        "\n",
        "  pred = tf.argmax(out, 1)\n",
        "\n",
        "  return pred, z_out\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "  epoch_loss = 0\n",
        "  nb_batches = int(nb_train/batch_size)\n",
        "  for i in range(nb_batches):\n",
        "    x = x_train[i*batch_size : (i+1)*batch_size, :]\n",
        "    y = y_train[i*batch_size : (i+1)*batch_size]\n",
        "    y_onehot = tf.one_hot(y, nb_classes)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      _, z_out = runNN(x)\n",
        "\n",
        "      loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=z_out, labels=y_onehot))\n",
        "\n",
        "    w1_g, w2_g, wout_g, b1_g, b2_g, bout_g = tape.gradient(loss, [w['1'], w['2'], w['out'], b['1'], b['2'], b['out']])\n",
        "    opt.apply_gradients(zip([w1_g, w2_g, wout_g, b1_g, b2_g, bout_g], [w['1'], w['2'], w['out'], b['1'], b['2'], b['out']]))\n",
        "\n",
        "    epoch_loss += loss\n",
        "\n",
        "  epoch_loss /= nb_train\n",
        "  print(f'Epoch: {epoch+1}/{nb_epochs}| Avg loss: {epoch_loss:.5f}')\n",
        "\n",
        "\n",
        "#Test\n",
        "test_data_path = '/content/drive/MyDrive/Colab Notebooks/Domaci2/titanic/test.csv'\n",
        "df_test = load_clean_data(test_data_path)\n",
        "\n",
        "y_test = df_test.Survived.to_numpy()\n",
        "df_test.drop(['Survived'], axis=1, inplace=True)\n",
        "x_test = df_test.to_numpy()\n",
        "\n",
        "pred, _ = runNN(x_test)\n",
        "pred_correct = tf.equal(pred, y_test)\n",
        "accuracy = tf.reduce_mean(tf.cast(pred_correct, tf.float32))\n",
        "\n",
        "print(f'Test acc: {accuracy:.3f}')\n",
        "\n",
        "#Evaluacija modela\n",
        "predictions = pred\n",
        "positive_predictions = np.where(predictions == 1)\n",
        "negative_predictions = np.where(predictions == 0)\n",
        "\n",
        "TP = np.sum((y_test[positive_predictions] == 1).astype(int))\n",
        "TN = np.sum((y_test[negative_predictions] == 0).astype(int))\n",
        "FP = np.sum((y_test[positive_predictions] == 0).astype(int))\n",
        "FN = np.sum((y_test[negative_predictions] == 1).astype(int))\n",
        "\n",
        "nb_test = y_test.shape[0]\n",
        "print('Total test samples: {}'.format(nb_test))\n",
        "print('TP = {}, TN = {}, FP = {}, FN = {}'.format(TP, TN, FP, FN))\n",
        "\n",
        "\n",
        "accuracy = (TP + TN) / nb_test\n",
        "precision = TP / (TP + FP) if TP + FP > 0 else -1\n",
        "recall = TP / (TP + FN) if TP + FN > 0 else -1\n",
        "print('A = {:.2f}, P = {:.2f}, R = {:.2f}'.format(accuracy, precision, recall))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S5QJFmRdqsu",
        "outputId": "cd91b1a9-4741-4978-d259-80f04e8a3b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100| Avg loss: 18.12544\n",
            "Epoch: 2/100| Avg loss: 11.86954\n",
            "Epoch: 3/100| Avg loss: 4.87732\n",
            "Epoch: 4/100| Avg loss: 3.65341\n",
            "Epoch: 5/100| Avg loss: 2.85218\n",
            "Epoch: 6/100| Avg loss: 3.03508\n",
            "Epoch: 7/100| Avg loss: 2.55755\n",
            "Epoch: 8/100| Avg loss: 2.37131\n",
            "Epoch: 9/100| Avg loss: 1.24611\n",
            "Epoch: 10/100| Avg loss: 1.69271\n",
            "Epoch: 11/100| Avg loss: 2.19549\n",
            "Epoch: 12/100| Avg loss: 1.66027\n",
            "Epoch: 13/100| Avg loss: 3.02369\n",
            "Epoch: 14/100| Avg loss: 0.77118\n",
            "Epoch: 15/100| Avg loss: 0.92814\n",
            "Epoch: 16/100| Avg loss: 1.86742\n",
            "Epoch: 17/100| Avg loss: 2.40074\n",
            "Epoch: 18/100| Avg loss: 2.63193\n",
            "Epoch: 19/100| Avg loss: 1.33265\n",
            "Epoch: 20/100| Avg loss: 1.84754\n",
            "Epoch: 21/100| Avg loss: 3.27453\n",
            "Epoch: 22/100| Avg loss: 1.22944\n",
            "Epoch: 23/100| Avg loss: 2.70242\n",
            "Epoch: 24/100| Avg loss: 0.98001\n",
            "Epoch: 25/100| Avg loss: 2.46620\n",
            "Epoch: 26/100| Avg loss: 1.05824\n",
            "Epoch: 27/100| Avg loss: 2.03703\n",
            "Epoch: 28/100| Avg loss: 1.45463\n",
            "Epoch: 29/100| Avg loss: 3.21853\n",
            "Epoch: 30/100| Avg loss: 1.21063\n",
            "Epoch: 31/100| Avg loss: 2.06862\n",
            "Epoch: 32/100| Avg loss: 1.44036\n",
            "Epoch: 33/100| Avg loss: 3.30280\n",
            "Epoch: 34/100| Avg loss: 3.72740\n",
            "Epoch: 35/100| Avg loss: 3.98133\n",
            "Epoch: 36/100| Avg loss: 3.19609\n",
            "Epoch: 37/100| Avg loss: 2.19755\n",
            "Epoch: 38/100| Avg loss: 0.85816\n",
            "Epoch: 39/100| Avg loss: 0.76331\n",
            "Epoch: 40/100| Avg loss: 1.68751\n",
            "Epoch: 41/100| Avg loss: 0.70486\n",
            "Epoch: 42/100| Avg loss: 1.15577\n",
            "Epoch: 43/100| Avg loss: 1.76845\n",
            "Epoch: 44/100| Avg loss: 0.98836\n",
            "Epoch: 45/100| Avg loss: 1.95515\n",
            "Epoch: 46/100| Avg loss: 0.98431\n",
            "Epoch: 47/100| Avg loss: 1.43701\n",
            "Epoch: 48/100| Avg loss: 1.06546\n",
            "Epoch: 49/100| Avg loss: 1.13675\n",
            "Epoch: 50/100| Avg loss: 1.16245\n",
            "Epoch: 51/100| Avg loss: 1.56986\n",
            "Epoch: 52/100| Avg loss: 4.18140\n",
            "Epoch: 53/100| Avg loss: 3.90241\n",
            "Epoch: 54/100| Avg loss: 3.86548\n",
            "Epoch: 55/100| Avg loss: 4.60604\n",
            "Epoch: 56/100| Avg loss: 3.95466\n",
            "Epoch: 57/100| Avg loss: 4.61957\n",
            "Epoch: 58/100| Avg loss: 2.54326\n",
            "Epoch: 59/100| Avg loss: 1.05073\n",
            "Epoch: 60/100| Avg loss: 0.80255\n",
            "Epoch: 61/100| Avg loss: 0.76032\n",
            "Epoch: 62/100| Avg loss: 0.70318\n",
            "Epoch: 63/100| Avg loss: 1.80680\n",
            "Epoch: 64/100| Avg loss: 1.90372\n",
            "Epoch: 65/100| Avg loss: 0.93446\n",
            "Epoch: 66/100| Avg loss: 0.64464\n",
            "Epoch: 67/100| Avg loss: 1.40643\n",
            "Epoch: 68/100| Avg loss: 2.42324\n",
            "Epoch: 69/100| Avg loss: 0.82922\n",
            "Epoch: 70/100| Avg loss: 0.62049\n",
            "Epoch: 71/100| Avg loss: 0.56183\n",
            "Epoch: 72/100| Avg loss: 0.78591\n",
            "Epoch: 73/100| Avg loss: 0.54464\n",
            "Epoch: 74/100| Avg loss: 1.99851\n",
            "Epoch: 75/100| Avg loss: 0.99798\n",
            "Epoch: 76/100| Avg loss: 0.92706\n",
            "Epoch: 77/100| Avg loss: 1.12549\n",
            "Epoch: 78/100| Avg loss: 1.39805\n",
            "Epoch: 79/100| Avg loss: 3.61955\n",
            "Epoch: 80/100| Avg loss: 4.02886\n",
            "Epoch: 81/100| Avg loss: 4.03724\n",
            "Epoch: 82/100| Avg loss: 4.39634\n",
            "Epoch: 83/100| Avg loss: 4.36779\n",
            "Epoch: 84/100| Avg loss: 3.51032\n",
            "Epoch: 85/100| Avg loss: 4.88985\n",
            "Epoch: 86/100| Avg loss: 2.17808\n",
            "Epoch: 87/100| Avg loss: 1.12977\n",
            "Epoch: 88/100| Avg loss: 0.80549\n",
            "Epoch: 89/100| Avg loss: 0.87404\n",
            "Epoch: 90/100| Avg loss: 1.64204\n",
            "Epoch: 91/100| Avg loss: 0.74071\n",
            "Epoch: 92/100| Avg loss: 2.42629\n",
            "Epoch: 93/100| Avg loss: 1.75097\n",
            "Epoch: 94/100| Avg loss: 0.85030\n",
            "Epoch: 95/100| Avg loss: 1.01020\n",
            "Epoch: 96/100| Avg loss: 0.59307\n",
            "Epoch: 97/100| Avg loss: 0.64179\n",
            "Epoch: 98/100| Avg loss: 2.61213\n",
            "Epoch: 99/100| Avg loss: 1.07045\n",
            "Epoch: 100/100| Avg loss: 0.58261\n",
            "Test acc: 0.828\n",
            "Total test samples: 331\n",
            "TP = 99, TN = 175, FP = 29, FN = 28\n",
            "A = 0.83, P = 0.77, R = 0.78\n"
          ]
        }
      ]
    }
  ]
}